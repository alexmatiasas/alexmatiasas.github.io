---
layout: single
title: "Sentiment analysis"
excerpt: "Using machine learning to detect sentiments."
header:
  teaser: "/assets/images/sentiments.jpg"  # Imagen de portada del proyecto
  overlay_image: "/assets/images/sentiments.jpg"  # Imagen de portada del proyecto
  overlay_filter: linear-gradient(rgba(255, 0, 0, 0.5), rgba(0, 255, 255, 0.5))
  overlay_filter: "0.5"
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
date: 2023-10-27
sidebar:
  title: "Projects"
  nav: "projects-sidebar"
toc: true
toc_sticky: true
toc_label: "Contents"
author_profile: true
related: true
categories:
  - Natural Language Processing (NLP)
tags:
  - Sentiment Analysis
  - Natural Language Processing (NLP)
  - Text Processing
  - Classification
  - Scikit-Learn
  - Python
---

# Sentiment Analysis Project

## Project Overview
This project uses machine learning algorithms to detect fraudulent transactions.

<!-- 

-  Construye un modelo que analice el sentimiento de reseñas de productos o tweets.
	•	Ingeniería de características: Trabaja en mejorar la calidad de los datos de entrada.
	•	Tuning de hiperparámetros: Experimenta con la búsqueda de hiperparámetros (Grid Search, Random Search) y técnicas como optimización bayesiana.

 -->

<!-- 

Puedes encontrar datasets en plataformas como Kaggle, UCI Machine Learning Repository, o los datasets de Google.

b) Estructura tus proyectos profesionalmente

Organiza bien tus proyectos para que otros puedan seguirlos fácilmente. Usa notebooks como Jupyter para exploración, pero también incluye scripts bien organizados cuando sea necesario para demostrar buenas prácticas de ingeniería.

c) Enfócate en la escalabilidad y eficiencia

Google se preocupa mucho por la escalabilidad. Trabaja en proyectos donde puedas manejar grandes volúmenes de datos o resolver problemas de optimización. Por ejemplo:

	•	Procesa datasets grandes con Apache Spark o Dask.
	•	Implementa modelos eficientes y evalúa su desempeño en entornos de producción simulados.

d) Deploy en la nube

Aprender a desplegar modelos en la nube es un gran plus. Familiarízate con Google Cloud Platform (GCP) o AWS. Puedes desplegar modelos con servicios como TensorFlow Serving, Flask o FastAPI en plataformas como Google Cloud AI o AWS SageMaker.

e) Documentación y GitHub

Publica tus proyectos en GitHub con una buena documentación. Esto mostrará a los reclutadores que sabes cómo trabajar en entornos colaborativos y que eres organizado. 
 -->

<!-- 
a) Competencias en algoritmos y estructuras de datos

Google es famoso por hacer preguntas de algoritmos y estructuras de datos en las entrevistas. Dedica tiempo a practicar en plataformas como:

	•	LeetCode
	•	HackerRank
	•	Codeforces

Trabaja en problemas de búsqueda binaria, árboles, listas enlazadas, algoritmos de grafos y dinámica de programación.

b) Sistemas distribuidos y arquitectura de software

Para roles en Machine Learning Engineer, es importante comprender cómo escalar y desplegar sistemas de ML. Familiarízate con:

	•	Sistemas distribuidos
	•	Microservicios
	•	Gestión de pipelines de datos

Usa frameworks como Airflow o Kubeflow para orquestar pipelines de machine learning.

c) Entrevistas técnicas

Las entrevistas en Google suelen incluir:

	1.	Coding challenges: Preguntas de algoritmos y estructuras de datos.
	2.	Preguntas de diseño de sistemas: Piensa en cómo diseñarías un sistema distribuido escalable para algún servicio de Google (como Gmail o Google Maps).
	3.	Entrevistas de machine learning: Prepárate para explicar conceptos teóricos como regularización, overfitting, gradient descent, etc.
	4.	Behavioral interviews: Prepara respuestas para preguntas sobre cómo manejas problemas complejos, trabajas en equipo o cómo enfrentas situaciones de incertidumbre.

5. Recursos útiles

	•	Coursera: Cursos como el de Deep Learning Specialization de Andrew Ng.
	•	Fast.ai: Excelente para aprender deep learning.
	•	Kaggle: Participa en competiciones para aplicar tus habilidades.
	•	Machine Learning de Google Cloud: Conoce las herramientas que usa Google.
	•	Libros recomendados: “Deep Learning with Python”, “Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow”.
 -->


<!-- 
3. Big Data y Procesamiento Distribuido

	•	Apache Spark: Es una de las herramientas más utilizadas para procesar grandes volúmenes de datos. Spark permite el procesamiento distribuido en clusters, lo cual es fundamental cuando trabajas con datasets que no caben en la memoria de una sola máquina.
	•	Hadoop: Aunque menos usado en ciencia de datos moderna, sigue siendo relevante para almacenar grandes volúmenes de datos no estructurados.
 -->

 <!-- 
 6. Cloud Computing y Deploy de Modelos

	•	Google Cloud Platform (GCP): Familiarízate con GCP, ya que Google lo usa internamente. Aprende a usar Google AI Platform, BigQuery (para grandes volúmenes de datos) y Cloud Storage.
	•	AWS o Microsoft Azure: Aunque Google Cloud es útil para Google, tener experiencia con AWS o Azure también es valioso.
	•	Docker: Para empaquetar aplicaciones y modelos en contenedores que pueden desplegarse en diferentes entornos sin problemas.
	•	Kubernetes: Para orquestar la implementación de aplicaciones en contenedores a gran escala.

Consejos:

	•	Aprende a desplegar modelos de machine learning como microservicios usando frameworks como Flask o FastAPI, y luego despliega esos servicios en la nube usando Google Cloud Run o AWS Lambda.
	•	Usa Docker para empaquetar tu modelo en un contenedor:

7. Herramientas de Versionado y Control de Proyectos

	•	Git y GitHub: El control de versiones es esencial en cualquier entorno profesional. Usa Git para gestionar versiones de tus proyectos y GitHub para colaborar y mostrar tus proyectos.
	•	DVC (Data Version Control): Para versionar datasets y experimentos, lo cual es útil en proyectos de machine learning donde los datos cambian constantemente.

Qué deberías saber:

	•	Python: Comprende conceptos fundamentales como estructuras de datos, funciones, y objetos, además de las bibliotecas clave para ciencia de datos (pandas, numpy).
	•	R: Familiarízate con la sintaxis básica, manipulación de datos (dplyr), y visualización de datos (ggplot2), especialmente si te interesa trabajar en análisis estadístico o académico.

4. Big Data y Procesamiento Distribuido: Apache Spark, Hadoop

Para procesar grandes volúmenes de datos, Apache Spark es el estándar.

Dónde aprender:

	•	Big Data Fundamentals (Coursera): Curso de introducción a Big Data y procesamiento distribuido.
	•	Apache Spark:
	•	Databricks Academy: Cursos gratuitos para Spark en Data Science.
	•	Big Data with Spark and Python (Udemy): Curso accesible que cubre Spark desde cero.
	•	Hadoop:
	•	The Ultimate Hands-On Hadoop (Udemy): Curso completo sobre Hadoop y su ecosistema.

Qué deberías saber:

	•	Spark: Conceptos de RDD, DataFrames en Spark, y cómo ejecutar transformaciones y acciones en datos distribuidos.
	•	Hadoop: Estructura del sistema HDFS, MapReduce, y ecosistema Hadoop.

5. SQL y Bases de Datos NoSQL: SQL, MongoDB

Es vital tener habilidades sólidas en SQL y comprensión de bases de datos NoSQL para datos no estructurados.

Dónde aprender:

	•	SQL:
	•	SQL for Data Science (Coursera): Curso enfocado en SQL para ciencia de datos.
	•	SQL Essentials (DataCamp): Curso de introducción con ejercicios interactivos.
	•	MongoDB:
	•	MongoDB University: Cursos gratuitos ofrecidos por MongoDB.
	•	DataCamp: Introduction to MongoDB: Curso introductorio sobre NoSQL.

Qué deberías saber:

	•	SQL: Conocer SELECT, JOIN, GROUP BY, subqueries y optimización de consultas.
	•	MongoDB: Sintaxis básica de consultas NoSQL y estructuras de documentos JSON.

7. Cloud Computing y Deploy de Modelos: GCP, AWS, Docker, Kubernetes

Las empresas de tecnología utilizan ampliamente los servicios en la nube y la contenedorización para desplegar modelos.

Dónde aprender:

	•	Google Cloud Platform (GCP):
	•	Google Cloud Training: Cursos gratuitos de Google, incluyendo introducción a BigQuery y AI Platform.
	•	Coursera: Data Engineering on GCP: Especialización en ingeniería de datos y ML en GCP.
	•	AWS:
	•	AWS Training and Certification: Cursos gratuitos y laboratorios prácticos.
	•	Docker y Kubernetes:
	•	Docker for Beginners (Udemy): Curso básico de Docker.
	•	Kubernetes for Data Scientists (Coursera): Curso sobre Kubernetes orientado a data science.

Qué deberías saber:

	•	GCP y AWS: Comprender los servicios de almacenamiento (BigQuery, S3), y AI Platform o SageMaker para modelado.
	•	Docker: Creación de contenedores, imágenes, y redes en Docker.
	•	Kubernetes: Conceptos básicos de orquestación de contenedores y creación de clusters.

8. Versionado y Control de Proyectos: Git, GitHub, DVC

Para trabajo colaborativo, Git es esencial, y DVC es útil para versionar datos y modelos.

Dónde aprender:

	•	Git y GitHub:
	•	Udacity: Version Control with Git: Curso gratuito de Git.
	•	Data Version Control (DVC):
	•	DVC Documentation: Documentación oficial de DVC, con ejemplos y tutoriales.

Qué deberías saber:

	•	Git: Flujo de trabajo con commits, ramas, merges, y resolución de conflictos.
	•	GitHub: Publicación de proyectos y colaboración.
	•	DVC: Versionado de datasets y pipelines en ML.
  -->


## Dataset and Preprocessing
Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.


<!-- 
Proyecto 1: Análisis de Sentimiento en Texto (NLP)

Objetivo: Crear un modelo que pueda clasificar reseñas de productos en positivo, negativo o neutral, usando análisis de sentimiento.

Paso a Paso

	1.	Definición del Problema y Recolección de Datos
	•	Objetivo: Crear un clasificador de sentimiento basado en reseñas de productos.
	•	Datos: Puedes usar datasets de reseñas, como el de Amazon o Yelp, disponibles en Kaggle, UCI Machine Learning Repository, o datasets de Hugging Face.
	2.	Exploración de Datos y Limpieza
	•	Realiza un análisis exploratorio inicial para entender la distribución de las clases (positivo, negativo, neutral).
	•	Limpieza de Texto:
	•	Convierte el texto a minúsculas, elimina signos de puntuación, URLs y caracteres especiales.
	•	Tokenización: Divide el texto en palabras individuales.
	•	Stop Words: Elimina palabras irrelevantes (como “el”, “de”, “un”).
	3.	Preprocesamiento Avanzado
	•	Vectorización: Usa CountVectorizer o TF-IDF para convertir texto a valores numéricos.
	•	Embeddings: Para mejorar el desempeño, usa embeddings preentrenados como Word2Vec o GloVe.
	•	Modelo Transformer (opcional): Si deseas un enfoque más avanzado, utiliza modelos preentrenados como BERT de Hugging Face.
	4.	Entrenamiento del Modelo
	•	Prueba varios modelos, como Naive Bayes, Support Vector Machines (SVM) y Redes Neuronales.
	•	Compara el desempeño de los modelos usando métricas como accuracy, precision, recall y f1-score.
	5.	Evaluación y Mejora
	•	Evalúa el modelo usando una matriz de confusión y métricas de clasificación.
	•	Optimización de hiperparámetros: Usa técnicas como Grid Search o Random Search para mejorar el rendimiento.
	6.	Despliegue y Visualización
	•	Crea un API con Flask o FastAPI para recibir texto como entrada y devolver el sentimiento.
	•	Crea un dashboard interactivo (usando Plotly Dash o Streamlit) para mostrar visualmente las predicciones y métricas.
 -->

## Model and Evaluation
Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.

<!-- 
Consejos Generales para el Desarrollo de Proyectos de Ciencia de Datos

	1.	Documenta Todo el Proceso: Desde la recolección de datos hasta el despliegue, anota decisiones y explica el porqué de cada elección (puedes usar Markdown en Jupyter Notebooks).
	2.	Itera en el Desarrollo del Modelo: Enfócate primero en construir un modelo base y luego optimízalo. Asegúrate de evaluar cada cambio.
	3.	Visualización y Comunicación de Resultados: La visualización es clave en ciencia de datos. Para los tres proyectos, usa gráficos para mostrar el rendimiento del modelo, ejemplos de predicciones y métricas.
	4.	Manejo de Datos: Almacena los datos de entrenamiento y prueba de manera organizada para poder acceder y reutilizar los mismos conjuntos en experimentos futuros.
	5.	Versión de Datos y Modelos: Utiliza DVC o Git-LFS si trabajas con grandes volúmenes de datos. Esto te ayudará a rastrear cambios y replicar experimentos.
	6.	Prepara tu Portafolio: Publica el código en GitHub con una explicación detallada de cada proyecto, incluyendo README y guías de instalación y uso. Esto demuestra tus habilidades técnicas y tu capacidad para documentar y comunicar.
	7.	Despliegue y Mantenimiento: Experimenta desplegando tus modelos en la nube (usando servicios gratuitos de GCP o AWS) y mantén tu API activa y actualizada para mostrarla en entrevistas o a reclutadores.
 -->

<!-- // ![ROC Curve](/assets/images/fraud_detection_roc.png) -->

## Conclusions
Summarize the results and future steps.

<!-- 
Proyecto: Clasificación de Sentimiento en Reseñas de Productos (NLP)

Objetivo: Construir un modelo de machine learning para clasificar reseñas de productos en positivo, neutral o negativo, utilizando un enfoque de Procesamiento de Lenguaje Natural (NLP).

Paso 1: Definición del Problema y Recopilación de Datos

	1.	Definir el objetivo:
	•	Queremos que el modelo clasifique una reseña en tres categorías: positivo, negativo o neutral.
	•	Ejemplo de uso: una empresa podría usar este modelo para monitorear la satisfacción del cliente en tiempo real.
	2.	Obtén los datos:
	•	Usaremos datasets que contengan reseñas etiquetadas. Ejemplos incluyen:
	•	Amazon Customer Reviews en Kaggle.
	•	Yelp Review Dataset.
	•	IMDb Movie Reviews.
	3.	Explora el dataset y define el flujo de procesamiento de datos:
	•	Carga el dataset y visualiza las primeras filas.
	•	Identifica las columnas importantes: texto de la reseña y etiqueta (positiva, negativa, neutral).

Código para explorar el dataset:
import pandas as pd

# Cargar el dataset
df = pd.read_csv("reviews.csv")  # Cambia esto a tu dataset

# Ver las primeras filas
print(df.head())

# Información general sobre el dataset
print(df.info())

Paso 2: Exploración y Limpieza de Datos

	1.	Exploración de datos:
	•	Analiza la distribución de clases (positivo, negativo, neutral). Esto te ayudará a identificar si el dataset está balanceado o si será necesario algún ajuste.

print(df['sentiment'].value_counts())  # Ver distribución de clases

	2.	Limpieza del texto:
	•	Convertir el texto a minúsculas para normalizar.
	•	Eliminar signos de puntuación, caracteres especiales, URLs, y emojis si es necesario.
	•	Tokenizar el texto (dividirlo en palabras).

import re
import nltk
from nltk.corpus import stopwords

# Descargar stop words
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def limpiar_texto(texto):
    # Convertir a minúsculas
    texto = texto.lower()
    # Eliminar URLs y caracteres especiales
    texto = re.sub(r"http\S+|www\S+|https\S+", '', texto, flags=re.MULTILINE)
    texto = re.sub(r'\@w+|\#','', texto)
    # Eliminar signos de puntuación
    texto = re.sub(r'[^\w\s]', '', texto)
    # Eliminar stop words
    texto = ' '.join([palabra for palabra in texto.split() if palabra not in stop_words])
    return texto

# Aplicar limpieza al dataset
df['cleaned_text'] = df['review_text'].apply(limpiar_texto)

Paso 3: Preprocesamiento de Datos

	1.	Vectorización del texto:
	•	Convierte el texto en una representación numérica. Para un proyecto robusto, es ideal comenzar con TF-IDF o probar embeddings preentrenados como Word2Vec o GloVe.
	•	TF-IDF transforma cada documento en un vector de palabras donde cada palabra está ponderada por su frecuencia inversa en el corpus.

from sklearn.feature_extraction.text import TfidfVectorizer

# Vectorización TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['cleaned_text']).toarray()
y = df['sentiment']  # La columna de etiquetas (positivo, negativo, neutral)

	2.	División de los datos:
	•	Divide los datos en conjuntos de entrenamiento y prueba.

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

Paso 4: Selección y Entrenamiento del Modelo

	1.	Modelos Base:
	•	Comienza con modelos como Naive Bayes, Support Vector Machines (SVM) y Random Forest para evaluar el desempeño inicial.

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Entrenar modelo de Naive Bayes
modelo_nb = MultinomialNB()
modelo_nb.fit(X_train, y_train)

# Predicción y evaluación
y_pred = modelo_nb.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

	2.	Optimización de Hiperparámetros:
	•	Usa GridSearchCV o RandomizedSearchCV para mejorar el rendimiento del modelo con los parámetros óptimos.

from sklearn.model_selection import GridSearchCV

parametros = {'alpha': [0.1, 0.5, 1.0]}  # Ejemplo de parámetros para Naive Bayes
grid_search = GridSearchCV(estimator=modelo_nb, param_grid=parametros, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Mejor hiperparámetro:", grid_search.best_params_)


3.	Modelo Avanzado: Embeddings y Redes Neuronales (opcional)
	•	Para mejorar el rendimiento, usa embeddings preentrenados y una red neuronal simple.
	•	También puedes probar transformers (como BERT) para una mayor precisión.

Paso 5: Evaluación y Optimización Final

	1.	Evaluación detallada:
	•	Usa una matriz de confusión para visualizar errores comunes y ajustar el modelo si es necesario.

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Crear matriz de confusión
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

	2.	Ajustes finales:
	•	Basado en la matriz de confusión, ajusta el preprocesamiento o prueba otros algoritmos si ciertos errores son recurrentes.

Paso 6: Despliegue del Modelo

	1.	Crear una API de Inferencia:
	•	Usa Flask o FastAPI para crear una API que reciba texto y devuelva la predicción del sentimiento.

from flask import Flask, request, jsonify
import joblib

# Guardar modelo entrenado
joblib.dump(modelo_nb, 'sentiment_model.pkl')

# Crear la API con Flask
app = Flask(__name__)

# Cargar modelo
model = joblib.load('sentiment_model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json(force=True)
    review_text = data['text']
    cleaned_text = limpiar_texto(review_text)  # Usa la función de limpieza que creamos
    vectorized_text = vectorizer.transform([cleaned_text]).toarray()
    prediction = model.predict(vectorized_text)
    return jsonify({'sentiment': prediction[0]})

if __name__ == '__main__':
    app.run(debug=True)

	2.	Despliega en la nube:
	•	Puedes usar Google Cloud Run, AWS Lambda o Heroku para alojar tu API y hacerla accesible de manera pública.

Paso 7: Creación de Dashboard Interactivo

	1.	Dashboard para Visualización de Resultados:
	•	Usa Streamlit o Plotly Dash para crear una interfaz de usuario donde los usuarios puedan escribir una reseña y ver la predicción.

import streamlit as st
import joblib

# Cargar el modelo
model = joblib.load('sentiment_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')  # Cargar vectorizador si se guardó

# Crear la interfaz en Streamlit
st.title("Clasificación de Sentimiento en Reseñas")
review_text = st.text_area("Ingrese su reseña aquí:")

if st.button("Clasificar"):
    cleaned_text = limpiar_texto(review_text)
    vectorized_text = vectorizer.transform([cleaned_text]).toarray()
    prediction = model.predict(vectorized_text)
    st.write(f"Sentimiento Predicho: {prediction[0]}")
 -->