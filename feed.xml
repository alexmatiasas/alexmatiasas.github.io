<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://alexmatiasas.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://alexmatiasas.github.io/" rel="alternate" type="text/html" /><updated>2025-04-20T09:43:17-06:00</updated><id>https://alexmatiasas.github.io/feed.xml</id><title type="html">Manuel Alejandro Matías Astorga Data Science Portfolio</title><subtitle>A showcase of my Data Science and Machine Learning projects.</subtitle><author><name>Manuel Alejandro Matías Astorga</name></author><entry><title type="html"></title><link href="https://alexmatiasas.github.io/2016-07-20-fraud-detection/" rel="alternate" type="text/html" title="" /><published>2025-04-20T09:43:17-06:00</published><updated>2025-04-20T09:43:17-06:00</updated><id>https://alexmatiasas.github.io/2016-07-20-fraud-detection</id><content type="html" xml:base="https://alexmatiasas.github.io/2016-07-20-fraud-detection/"><![CDATA[<h2 id="project-overview">Project Overview</h2>
<p>This project uses machine learning algorithms to detect fraudulent transactions.</p>

<!-- 
	•	Ingeniería de características: Trabaja en mejorar la calidad de los datos de entrada.
	•	Tuning de hiperparámetros: Experimenta con la búsqueda de hiperparámetros (Grid Search, Random Search) y técnicas como optimización bayesiana.

 -->

<h2 id="dataset-and-preprocessing">Dataset and Preprocessing</h2>
<p>Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.</p>

<h2 id="model-and-evaluation">Model and Evaluation</h2>
<p>Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.</p>

<!-- // ![ROC Curve](/assets/images/fraud_detection_roc.png) -->

<h2 id="conclusions">Conclusions</h2>
<p>Summarize the results and future steps.</p>]]></content><author><name>Manuel Alejandro Matías Astorga</name></author></entry><entry><title type="html"></title><link href="https://alexmatiasas.github.io/2016-07-20-image-classification/" rel="alternate" type="text/html" title="" /><published>2025-04-20T09:43:17-06:00</published><updated>2025-04-20T09:43:17-06:00</updated><id>https://alexmatiasas.github.io/2016-07-20-image-classification</id><content type="html" xml:base="https://alexmatiasas.github.io/2016-07-20-image-classification/"><![CDATA[<h1 id="language-model-project">Language model Project</h1>

<h2 id="project-overview">Project Overview</h2>
<p>This project uses machine learning algorithms to construct an image classificator.</p>

<!-- 

- Utiliza redes neuronales convolucionales (CNN) para clasificar imágenes en diferentes categorías.
	•	Ingeniería de características: Trabaja en mejorar la calidad de los datos de entrada.
	•	Tuning de hiperparámetros: Experimenta con la búsqueda de hiperparámetros (Grid Search, Random Search) y técnicas como optimización bayesiana.

 -->

<h2 id="dataset-and-preprocessing">Dataset and Preprocessing</h2>
<p>Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.</p>

<!-- 
Proyecto 2: Clasificación de Imágenes de Gatos y Perros (Computer Vision)

Objetivo: Construir un clasificador que identifique si una imagen contiene un gato o un perro usando redes neuronales convolucionales (CNN).

Paso a Paso

	1.	Definición del Problema y Recolección de Datos
	•	Objetivo: Distinguir entre imágenes de gatos y perros.
	•	Datos: Usa datasets de Kaggle como el de gatos y perros de Microsoft (disponible en la plataforma Kaggle).
	2.	Exploración de Datos
	•	Examina el dataset para ver ejemplos de cada clase (gatos y perros).
	•	Balance de clases: Revisa si hay una cantidad similar de imágenes de cada categoría.
	3.	Preprocesamiento de Imágenes
	•	Redimensionamiento: Redimensiona las imágenes a un tamaño manejable (ej., 128x128 píxeles).
	•	Normalización: Escala los valores de píxeles entre 0 y 1 para facilitar el entrenamiento.
	•	Data Augmentation: Aplica transformaciones (rotación, zoom, desplazamiento) para mejorar la generalización del modelo.
	4.	Arquitectura del Modelo CNN
	•	Usa una arquitectura CNN básica con capas convolucionales, de pooling y completamente conectadas.
	•	Transfer Learning: Experimenta con modelos preentrenados como VGG16 o ResNet para mejorar el rendimiento.
	5.	Entrenamiento del Modelo
	•	Usa una división en entrenamiento, validación y prueba.
	•	Ajusta hiperparámetros clave como batch size, learning rate, y número de épocas.
	6.	Evaluación del Modelo
	•	Usa métricas como accuracy y curvas ROC/AUC para evaluar el rendimiento.
	•	Visualiza predicciones incorrectas para analizar posibles mejoras en el modelo.
	7.	Despliegue y Visualización
	•	API de Inferencia: Crea una API para cargar una imagen y recibir la predicción (si es un gato o un perro).
	•	Crea un dashboard de clasificación interactivo donde se muestre la imagen cargada y la predicción del modelo.
 -->

<h2 id="model-and-evaluation">Model and Evaluation</h2>
<p>Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.</p>

<!-- // ![ROC Curve](/assets/images/fraud_detection_roc.png) -->

<h2 id="conclusions">Conclusions</h2>
<p>Summarize the results and future steps.</p>]]></content><author><name>Manuel Alejandro Matías Astorga</name></author></entry><entry><title type="html"></title><link href="https://alexmatiasas.github.io/2016-07-20-image-recognition/" rel="alternate" type="text/html" title="" /><published>2025-04-20T09:43:17-06:00</published><updated>2025-04-20T09:43:17-06:00</updated><id>https://alexmatiasas.github.io/2016-07-20-image-recognition</id><content type="html" xml:base="https://alexmatiasas.github.io/2016-07-20-image-recognition/"><![CDATA[<h1 id="image-recognition-project">Image recognition Project</h1>

<h2 id="project-overview">Project Overview</h2>
<p>This project uses machine learning algorithms to construct a recognizer of images.</p>

<!-- 

- Usa OpenCV para reconocimiento facial o para detección de emociones.
	•	Ingeniería de características: Trabaja en mejorar la calidad de los datos de entrada.
	•	Tuning de hiperparámetros: Experimenta con la búsqueda de hiperparámetros (Grid Search, Random Search) y técnicas como optimización bayesiana.

 -->

<h2 id="dataset-and-preprocessing">Dataset and Preprocessing</h2>
<p>Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.</p>

<h2 id="model-and-evaluation">Model and Evaluation</h2>
<p>Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.</p>

<!-- // ![ROC Curve](/assets/images/fraud_detection_roc.png) -->

<h2 id="conclusions">Conclusions</h2>
<p>Summarize the results and future steps.</p>]]></content><author><name>Manuel Alejandro Matías Astorga</name></author></entry><entry><title type="html"></title><link href="https://alexmatiasas.github.io/2016-07-20-language-model/" rel="alternate" type="text/html" title="" /><published>2025-04-20T09:43:17-06:00</published><updated>2025-04-20T09:43:17-06:00</updated><id>https://alexmatiasas.github.io/2016-07-20-language-model</id><content type="html" xml:base="https://alexmatiasas.github.io/2016-07-20-language-model/"><![CDATA[<h1 id="language-model-project">Language model Project</h1>

<h2 id="project-overview">Project Overview</h2>
<p>This project uses machine learning algorithms to construct a language model.</p>

<!-- 

- Trabaja en proyectos como clasificación de texto, chatbots o implementa un modelo de transformers como GPT.
	•	Ingeniería de características: Trabaja en mejorar la calidad de los datos de entrada.
	•	Tuning de hiperparámetros: Experimenta con la búsqueda de hiperparámetros (Grid Search, Random Search) y técnicas como optimización bayesiana.

 -->

<h2 id="dataset-and-preprocessing">Dataset and Preprocessing</h2>
<p>Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.</p>

<h2 id="model-and-evaluation">Model and Evaluation</h2>
<p>Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.</p>

<!-- // ![ROC Curve](/assets/images/fraud_detection_roc.png) -->

<h2 id="conclusions">Conclusions</h2>
<p>Summarize the results and future steps.</p>]]></content><author><name>Manuel Alejandro Matías Astorga</name></author></entry><entry><title type="html"></title><link href="https://alexmatiasas.github.io/2016-07-20-object-detection/" rel="alternate" type="text/html" title="" /><published>2025-04-20T09:43:17-06:00</published><updated>2025-04-20T09:43:17-06:00</updated><id>https://alexmatiasas.github.io/2016-07-20-object-detection</id><content type="html" xml:base="https://alexmatiasas.github.io/2016-07-20-object-detection/"><![CDATA[<h1 id="object-detection-project">Object detection Project</h1>

<h2 id="project-overview">Project Overview</h2>
<p>This project uses machine learning algorithms to construct a text generator.</p>

<!-- 

- Implementa un modelo como YOLO o Faster R-CNN para detectar objetos en imágenes o videos.
	•	Ingeniería de características: Trabaja en mejorar la calidad de los datos de entrada.
	•	Tuning de hiperparámetros: Experimenta con la búsqueda de hiperparámetros (Grid Search, Random Search) y técnicas como optimización bayesiana.

 -->

<h2 id="dataset-and-preprocessing">Dataset and Preprocessing</h2>
<p>Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.</p>

<h2 id="model-and-evaluation">Model and Evaluation</h2>
<p>Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.</p>

<!-- // ![ROC Curve](/assets/images/fraud_detection_roc.png) -->

<h2 id="conclusions">Conclusions</h2>
<p>Summarize the results and future steps.</p>]]></content><author><name>Manuel Alejandro Matías Astorga</name></author></entry><entry><title type="html"></title><link href="https://alexmatiasas.github.io/2016-07-20-real-time-data-processing/" rel="alternate" type="text/html" title="" /><published>2025-04-20T09:43:17-06:00</published><updated>2025-04-20T09:43:17-06:00</updated><id>https://alexmatiasas.github.io/2016-07-20-real-time-data-processing</id><content type="html" xml:base="https://alexmatiasas.github.io/2016-07-20-real-time-data-processing/"><![CDATA[<h1 id="real-time-data-processing-project">Real time data processing Project</h1>

<h2 id="project-overview">Project Overview</h2>
<p>This project uses machine learning algorithms to analize real time data.</p>

<!-- 

-  Usa Apache Kafka o Spark Streaming para procesar datos en tiempo real.
	•	Ingeniería de características: Trabaja en mejorar la calidad de los datos de entrada.
	•	Tuning de hiperparámetros: Experimenta con la búsqueda de hiperparámetros (Grid Search, Random Search) y técnicas como optimización bayesiana.

 -->

<h2 id="dataset-and-preprocessing">Dataset and Preprocessing</h2>
<p>Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.</p>

<h2 id="model-and-evaluation">Model and Evaluation</h2>
<p>Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.</p>

<!-- // ![ROC Curve](/assets/images/fraud_detection_roc.png) -->

<h2 id="conclusions">Conclusions</h2>
<p>Summarize the results and future steps.</p>]]></content><author><name>Manuel Alejandro Matías Astorga</name></author></entry><entry><title type="html"></title><link href="https://alexmatiasas.github.io/2016-07-20-recomendation-system/" rel="alternate" type="text/html" title="" /><published>2025-04-20T09:43:17-06:00</published><updated>2025-04-20T09:43:17-06:00</updated><id>https://alexmatiasas.github.io/2016-07-20-recomendation-system</id><content type="html" xml:base="https://alexmatiasas.github.io/2016-07-20-recomendation-system/"><![CDATA[<h1 id="recommendation-system-project">Recommendation system Project</h1>

<h2 id="project-overview">Project Overview</h2>
<p>This project uses machine learning algorithms to recommend things.</p>

<!-- 

-  Usa matrices de similitud o modelos basados en contenido.
	•	Ingeniería de características: Trabaja en mejorar la calidad de los datos de entrada.
	•	Tuning de hiperparámetros: Experimenta con la búsqueda de hiperparámetros (Grid Search, Random Search) y técnicas como optimización bayesiana.

 -->

<h2 id="dataset-and-preprocessing">Dataset and Preprocessing</h2>
<p>Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.</p>

<!-- 
Proyecto 3: Sistema de Recomendación para Productos en E-commerce

Objetivo: Construir un sistema de recomendación que sugiera productos a los usuarios en función de sus interacciones anteriores.

Paso a Paso

	1.	Definición del Problema y Recolección de Datos
	•	Objetivo: Crear recomendaciones personalizadas de productos para los usuarios.
	•	Datos: Usa datasets de e-commerce como Retailrocket o Amazon Product Data en Kaggle o los datasets de MovieLens (para sistemas de recomendación basados en películas).
	2.	Exploración y Limpieza de Datos
	•	Análisis de patrones de usuario: Examina cómo interactúan los usuarios con los productos.
	•	Filtrado de datos: Elimina elementos o usuarios con pocas interacciones (para evitar ruido).
	3.	Selecciona un Enfoque de Recomendación
	•	Filtrado colaborativo: Basado en las interacciones de los usuarios.
	•	Filtrado basado en contenido: Basado en las características de los productos.
	•	Modelos híbridos: Combinan filtrado colaborativo y basado en contenido.
	4.	Desarrollo del Modelo
	•	Filtrado colaborativo:
	•	Usa matrices de similitud para calcular qué productos son similares a los que el usuario ha comprado o valorado.
	•	Implementa un modelo de factorización de matrices como SVD para crear recomendaciones.
	•	Basado en contenido:
	•	Vectoriza características de productos usando TF-IDF o CountVectorizer (en caso de productos con descripciones textuales).
	•	Calcula similitudes entre los productos usando cosine similarity.
	5.	Entrenamiento y Optimización del Modelo
	•	Optimiza la matriz de recomendaciones utilizando cross-validation para evitar sobreajuste.
	•	Experimenta con tuning de hiperparámetros en los modelos de recomendación.
	6.	Evaluación del Modelo
	•	Usa métricas como Precision@K y Recall@K para evaluar el modelo.
	•	Compara modelos de filtrado colaborativo con otros enfoques para encontrar el mejor ajuste.
	7.	Despliegue
	•	API de Recomendación: Despliega el sistema en una API que pueda recibir un usuario como entrada y devolver recomendaciones personalizadas.
	•	Dashboard Interactivo: Muestra las recomendaciones y la información del producto en una interfaz visual (por ejemplo, usando Streamlit o Plotly Dash).
 -->

<h2 id="model-and-evaluation">Model and Evaluation</h2>
<p>Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.</p>

<!-- // ![ROC Curve](/assets/images/fraud_detection_roc.png) -->

<h2 id="conclusions">Conclusions</h2>
<p>Summarize the results and future steps.</p>]]></content><author><name>Manuel Alejandro Matías Astorga</name></author></entry><entry><title type="html"></title><link href="https://alexmatiasas.github.io/2016-07-20-sentiment-analysis/" rel="alternate" type="text/html" title="" /><published>2025-04-20T09:43:17-06:00</published><updated>2025-04-20T09:43:17-06:00</updated><id>https://alexmatiasas.github.io/2016-07-20-sentiment-analysis</id><content type="html" xml:base="https://alexmatiasas.github.io/2016-07-20-sentiment-analysis/"><![CDATA[<h1 id="sentiment-analysis-project">Sentiment Analysis Project</h1>

<h2 id="project-overview">Project Overview</h2>
<p>This project uses machine learning algorithms to detect fraudulent transactions.</p>

<!-- 

-  Construye un modelo que analice el sentimiento de reseñas de productos o tweets.
	•	Ingeniería de características: Trabaja en mejorar la calidad de los datos de entrada.
	•	Tuning de hiperparámetros: Experimenta con la búsqueda de hiperparámetros (Grid Search, Random Search) y técnicas como optimización bayesiana.

 -->

<!-- 

Puedes encontrar datasets en plataformas como Kaggle, UCI Machine Learning Repository, o los datasets de Google.

b) Estructura tus proyectos profesionalmente

Organiza bien tus proyectos para que otros puedan seguirlos fácilmente. Usa notebooks como Jupyter para exploración, pero también incluye scripts bien organizados cuando sea necesario para demostrar buenas prácticas de ingeniería.

c) Enfócate en la escalabilidad y eficiencia

Google se preocupa mucho por la escalabilidad. Trabaja en proyectos donde puedas manejar grandes volúmenes de datos o resolver problemas de optimización. Por ejemplo:

	•	Procesa datasets grandes con Apache Spark o Dask.
	•	Implementa modelos eficientes y evalúa su desempeño en entornos de producción simulados.

d) Deploy en la nube

Aprender a desplegar modelos en la nube es un gran plus. Familiarízate con Google Cloud Platform (GCP) o AWS. Puedes desplegar modelos con servicios como TensorFlow Serving, Flask o FastAPI en plataformas como Google Cloud AI o AWS SageMaker.

e) Documentación y GitHub

Publica tus proyectos en GitHub con una buena documentación. Esto mostrará a los reclutadores que sabes cómo trabajar en entornos colaborativos y que eres organizado. 
 -->

<!-- 
a) Competencias en algoritmos y estructuras de datos

Google es famoso por hacer preguntas de algoritmos y estructuras de datos en las entrevistas. Dedica tiempo a practicar en plataformas como:

	•	LeetCode
	•	HackerRank
	•	Codeforces

Trabaja en problemas de búsqueda binaria, árboles, listas enlazadas, algoritmos de grafos y dinámica de programación.

b) Sistemas distribuidos y arquitectura de software

Para roles en Machine Learning Engineer, es importante comprender cómo escalar y desplegar sistemas de ML. Familiarízate con:

	•	Sistemas distribuidos
	•	Microservicios
	•	Gestión de pipelines de datos

Usa frameworks como Airflow o Kubeflow para orquestar pipelines de machine learning.

c) Entrevistas técnicas

Las entrevistas en Google suelen incluir:

	1.	Coding challenges: Preguntas de algoritmos y estructuras de datos.
	2.	Preguntas de diseño de sistemas: Piensa en cómo diseñarías un sistema distribuido escalable para algún servicio de Google (como Gmail o Google Maps).
	3.	Entrevistas de machine learning: Prepárate para explicar conceptos teóricos como regularización, overfitting, gradient descent, etc.
	4.	Behavioral interviews: Prepara respuestas para preguntas sobre cómo manejas problemas complejos, trabajas en equipo o cómo enfrentas situaciones de incertidumbre.

5. Recursos útiles

	•	Coursera: Cursos como el de Deep Learning Specialization de Andrew Ng.
	•	Fast.ai: Excelente para aprender deep learning.
	•	Kaggle: Participa en competiciones para aplicar tus habilidades.
	•	Machine Learning de Google Cloud: Conoce las herramientas que usa Google.
	•	Libros recomendados: “Deep Learning with Python”, “Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow”.
 -->

<!-- 
3. Big Data y Procesamiento Distribuido

	•	Apache Spark: Es una de las herramientas más utilizadas para procesar grandes volúmenes de datos. Spark permite el procesamiento distribuido en clusters, lo cual es fundamental cuando trabajas con datasets que no caben en la memoria de una sola máquina.
	•	Hadoop: Aunque menos usado en ciencia de datos moderna, sigue siendo relevante para almacenar grandes volúmenes de datos no estructurados.
 -->

<p>&lt;!–</p>
<ol>
  <li>
    <p>Cloud Computing y Deploy de Modelos</p>

    <p>•	Google Cloud Platform (GCP): Familiarízate con GCP, ya que Google lo usa internamente. Aprende a usar Google AI Platform, BigQuery (para grandes volúmenes de datos) y Cloud Storage.
•	AWS o Microsoft Azure: Aunque Google Cloud es útil para Google, tener experiencia con AWS o Azure también es valioso.
•	Docker: Para empaquetar aplicaciones y modelos en contenedores que pueden desplegarse en diferentes entornos sin problemas.
•	Kubernetes: Para orquestar la implementación de aplicaciones en contenedores a gran escala.</p>
  </li>
</ol>

<p>Consejos:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>•	Aprende a desplegar modelos de machine learning como microservicios usando frameworks como Flask o FastAPI, y luego despliega esos servicios en la nube usando Google Cloud Run o AWS Lambda.
•	Usa Docker para empaquetar tu modelo en un contenedor:
</code></pre></div></div>

<ol>
  <li>
    <p>Herramientas de Versionado y Control de Proyectos</p>

    <p>•	Git y GitHub: El control de versiones es esencial en cualquier entorno profesional. Usa Git para gestionar versiones de tus proyectos y GitHub para colaborar y mostrar tus proyectos.
 •	DVC (Data Version Control): Para versionar datasets y experimentos, lo cual es útil en proyectos de machine learning donde los datos cambian constantemente.</p>
  </li>
</ol>

<p>Qué deberías saber:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>•	Python: Comprende conceptos fundamentales como estructuras de datos, funciones, y objetos, además de las bibliotecas clave para ciencia de datos (pandas, numpy).
•	R: Familiarízate con la sintaxis básica, manipulación de datos (dplyr), y visualización de datos (ggplot2), especialmente si te interesa trabajar en análisis estadístico o académico.
</code></pre></div></div>

<ol>
  <li>Big Data y Procesamiento Distribuido: Apache Spark, Hadoop</li>
</ol>

<p>Para procesar grandes volúmenes de datos, Apache Spark es el estándar.</p>

<p>Dónde aprender:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>•	Big Data Fundamentals (Coursera): Curso de introducción a Big Data y procesamiento distribuido.
•	Apache Spark:
•	Databricks Academy: Cursos gratuitos para Spark en Data Science.
•	Big Data with Spark and Python (Udemy): Curso accesible que cubre Spark desde cero.
•	Hadoop:
•	The Ultimate Hands-On Hadoop (Udemy): Curso completo sobre Hadoop y su ecosistema.
</code></pre></div></div>

<p>Qué deberías saber:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>•	Spark: Conceptos de RDD, DataFrames en Spark, y cómo ejecutar transformaciones y acciones en datos distribuidos.
•	Hadoop: Estructura del sistema HDFS, MapReduce, y ecosistema Hadoop.
</code></pre></div></div>

<ol>
  <li>SQL y Bases de Datos NoSQL: SQL, MongoDB</li>
</ol>

<p>Es vital tener habilidades sólidas en SQL y comprensión de bases de datos NoSQL para datos no estructurados.</p>

<p>Dónde aprender:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>•	SQL:
•	SQL for Data Science (Coursera): Curso enfocado en SQL para ciencia de datos.
•	SQL Essentials (DataCamp): Curso de introducción con ejercicios interactivos.
•	MongoDB:
•	MongoDB University: Cursos gratuitos ofrecidos por MongoDB.
•	DataCamp: Introduction to MongoDB: Curso introductorio sobre NoSQL.
</code></pre></div></div>

<p>Qué deberías saber:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>•	SQL: Conocer SELECT, JOIN, GROUP BY, subqueries y optimización de consultas.
•	MongoDB: Sintaxis básica de consultas NoSQL y estructuras de documentos JSON.
</code></pre></div></div>

<ol>
  <li>Cloud Computing y Deploy de Modelos: GCP, AWS, Docker, Kubernetes</li>
</ol>

<p>Las empresas de tecnología utilizan ampliamente los servicios en la nube y la contenedorización para desplegar modelos.</p>

<p>Dónde aprender:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>•	Google Cloud Platform (GCP):
•	Google Cloud Training: Cursos gratuitos de Google, incluyendo introducción a BigQuery y AI Platform.
•	Coursera: Data Engineering on GCP: Especialización en ingeniería de datos y ML en GCP.
•	AWS:
•	AWS Training and Certification: Cursos gratuitos y laboratorios prácticos.
•	Docker y Kubernetes:
•	Docker for Beginners (Udemy): Curso básico de Docker.
•	Kubernetes for Data Scientists (Coursera): Curso sobre Kubernetes orientado a data science.
</code></pre></div></div>

<p>Qué deberías saber:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>•	GCP y AWS: Comprender los servicios de almacenamiento (BigQuery, S3), y AI Platform o SageMaker para modelado.
•	Docker: Creación de contenedores, imágenes, y redes en Docker.
•	Kubernetes: Conceptos básicos de orquestación de contenedores y creación de clusters.
</code></pre></div></div>

<ol>
  <li>Versionado y Control de Proyectos: Git, GitHub, DVC</li>
</ol>

<p>Para trabajo colaborativo, Git es esencial, y DVC es útil para versionar datos y modelos.</p>

<p>Dónde aprender:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>•	Git y GitHub:
•	Udacity: Version Control with Git: Curso gratuito de Git.
•	Data Version Control (DVC):
•	DVC Documentation: Documentación oficial de DVC, con ejemplos y tutoriales.
</code></pre></div></div>

<p>Qué deberías saber:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>•	Git: Flujo de trabajo con commits, ramas, merges, y resolución de conflictos.
•	GitHub: Publicación de proyectos y colaboración.
•	DVC: Versionado de datasets y pipelines en ML.   --&gt;
</code></pre></div></div>

<h2 id="dataset-and-preprocessing">Dataset and Preprocessing</h2>
<p>Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.</p>

<!-- 
Proyecto 1: Análisis de Sentimiento en Texto (NLP)

Objetivo: Crear un modelo que pueda clasificar reseñas de productos en positivo, negativo o neutral, usando análisis de sentimiento.

Paso a Paso

	1.	Definición del Problema y Recolección de Datos
	•	Objetivo: Crear un clasificador de sentimiento basado en reseñas de productos.
	•	Datos: Puedes usar datasets de reseñas, como el de Amazon o Yelp, disponibles en Kaggle, UCI Machine Learning Repository, o datasets de Hugging Face.
	2.	Exploración de Datos y Limpieza
	•	Realiza un análisis exploratorio inicial para entender la distribución de las clases (positivo, negativo, neutral).
	•	Limpieza de Texto:
	•	Convierte el texto a minúsculas, elimina signos de puntuación, URLs y caracteres especiales.
	•	Tokenización: Divide el texto en palabras individuales.
	•	Stop Words: Elimina palabras irrelevantes (como “el”, “de”, “un”).
	3.	Preprocesamiento Avanzado
	•	Vectorización: Usa CountVectorizer o TF-IDF para convertir texto a valores numéricos.
	•	Embeddings: Para mejorar el desempeño, usa embeddings preentrenados como Word2Vec o GloVe.
	•	Modelo Transformer (opcional): Si deseas un enfoque más avanzado, utiliza modelos preentrenados como BERT de Hugging Face.
	4.	Entrenamiento del Modelo
	•	Prueba varios modelos, como Naive Bayes, Support Vector Machines (SVM) y Redes Neuronales.
	•	Compara el desempeño de los modelos usando métricas como accuracy, precision, recall y f1-score.
	5.	Evaluación y Mejora
	•	Evalúa el modelo usando una matriz de confusión y métricas de clasificación.
	•	Optimización de hiperparámetros: Usa técnicas como Grid Search o Random Search para mejorar el rendimiento.
	6.	Despliegue y Visualización
	•	Crea un API con Flask o FastAPI para recibir texto como entrada y devolver el sentimiento.
	•	Crea un dashboard interactivo (usando Plotly Dash o Streamlit) para mostrar visualmente las predicciones y métricas.
 -->

<h2 id="model-and-evaluation">Model and Evaluation</h2>
<p>Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.</p>

<!-- 
Consejos Generales para el Desarrollo de Proyectos de Ciencia de Datos

	1.	Documenta Todo el Proceso: Desde la recolección de datos hasta el despliegue, anota decisiones y explica el porqué de cada elección (puedes usar Markdown en Jupyter Notebooks).
	2.	Itera en el Desarrollo del Modelo: Enfócate primero en construir un modelo base y luego optimízalo. Asegúrate de evaluar cada cambio.
	3.	Visualización y Comunicación de Resultados: La visualización es clave en ciencia de datos. Para los tres proyectos, usa gráficos para mostrar el rendimiento del modelo, ejemplos de predicciones y métricas.
	4.	Manejo de Datos: Almacena los datos de entrenamiento y prueba de manera organizada para poder acceder y reutilizar los mismos conjuntos en experimentos futuros.
	5.	Versión de Datos y Modelos: Utiliza DVC o Git-LFS si trabajas con grandes volúmenes de datos. Esto te ayudará a rastrear cambios y replicar experimentos.
	6.	Prepara tu Portafolio: Publica el código en GitHub con una explicación detallada de cada proyecto, incluyendo README y guías de instalación y uso. Esto demuestra tus habilidades técnicas y tu capacidad para documentar y comunicar.
	7.	Despliegue y Mantenimiento: Experimenta desplegando tus modelos en la nube (usando servicios gratuitos de GCP o AWS) y mantén tu API activa y actualizada para mostrarla en entrevistas o a reclutadores.
 -->

<!-- // ![ROC Curve](/assets/images/fraud_detection_roc.png) -->

<h2 id="conclusions">Conclusions</h2>
<p>Summarize the results and future steps.</p>

<!-- 
Proyecto: Clasificación de Sentimiento en Reseñas de Productos (NLP)

Objetivo: Construir un modelo de machine learning para clasificar reseñas de productos en positivo, neutral o negativo, utilizando un enfoque de Procesamiento de Lenguaje Natural (NLP).

Paso 1: Definición del Problema y Recopilación de Datos

	1.	Definir el objetivo:
	•	Queremos que el modelo clasifique una reseña en tres categorías: positivo, negativo o neutral.
	•	Ejemplo de uso: una empresa podría usar este modelo para monitorear la satisfacción del cliente en tiempo real.
	2.	Obtén los datos:
	•	Usaremos datasets que contengan reseñas etiquetadas. Ejemplos incluyen:
	•	Amazon Customer Reviews en Kaggle.
	•	Yelp Review Dataset.
	•	IMDb Movie Reviews.
	3.	Explora el dataset y define el flujo de procesamiento de datos:
	•	Carga el dataset y visualiza las primeras filas.
	•	Identifica las columnas importantes: texto de la reseña y etiqueta (positiva, negativa, neutral).

Código para explorar el dataset:
import pandas as pd

# Cargar el dataset
df = pd.read_csv("reviews.csv")  # Cambia esto a tu dataset

# Ver las primeras filas
print(df.head())

# Información general sobre el dataset
print(df.info())

Paso 2: Exploración y Limpieza de Datos

	1.	Exploración de datos:
	•	Analiza la distribución de clases (positivo, negativo, neutral). Esto te ayudará a identificar si el dataset está balanceado o si será necesario algún ajuste.

print(df['sentiment'].value_counts())  # Ver distribución de clases

	2.	Limpieza del texto:
	•	Convertir el texto a minúsculas para normalizar.
	•	Eliminar signos de puntuación, caracteres especiales, URLs, y emojis si es necesario.
	•	Tokenizar el texto (dividirlo en palabras).

import re
import nltk
from nltk.corpus import stopwords

# Descargar stop words
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def limpiar_texto(texto):
    # Convertir a minúsculas
    texto = texto.lower()
    # Eliminar URLs y caracteres especiales
    texto = re.sub(r"http\S+|www\S+|https\S+", '', texto, flags=re.MULTILINE)
    texto = re.sub(r'\@w+|\#','', texto)
    # Eliminar signos de puntuación
    texto = re.sub(r'[^\w\s]', '', texto)
    # Eliminar stop words
    texto = ' '.join([palabra for palabra in texto.split() if palabra not in stop_words])
    return texto

# Aplicar limpieza al dataset
df['cleaned_text'] = df['review_text'].apply(limpiar_texto)

Paso 3: Preprocesamiento de Datos

	1.	Vectorización del texto:
	•	Convierte el texto en una representación numérica. Para un proyecto robusto, es ideal comenzar con TF-IDF o probar embeddings preentrenados como Word2Vec o GloVe.
	•	TF-IDF transforma cada documento en un vector de palabras donde cada palabra está ponderada por su frecuencia inversa en el corpus.

from sklearn.feature_extraction.text import TfidfVectorizer

# Vectorización TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['cleaned_text']).toarray()
y = df['sentiment']  # La columna de etiquetas (positivo, negativo, neutral)

	2.	División de los datos:
	•	Divide los datos en conjuntos de entrenamiento y prueba.

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

Paso 4: Selección y Entrenamiento del Modelo

	1.	Modelos Base:
	•	Comienza con modelos como Naive Bayes, Support Vector Machines (SVM) y Random Forest para evaluar el desempeño inicial.

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Entrenar modelo de Naive Bayes
modelo_nb = MultinomialNB()
modelo_nb.fit(X_train, y_train)

# Predicción y evaluación
y_pred = modelo_nb.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

	2.	Optimización de Hiperparámetros:
	•	Usa GridSearchCV o RandomizedSearchCV para mejorar el rendimiento del modelo con los parámetros óptimos.

from sklearn.model_selection import GridSearchCV

parametros = {'alpha': [0.1, 0.5, 1.0]}  # Ejemplo de parámetros para Naive Bayes
grid_search = GridSearchCV(estimator=modelo_nb, param_grid=parametros, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Mejor hiperparámetro:", grid_search.best_params_)


3.	Modelo Avanzado: Embeddings y Redes Neuronales (opcional)
	•	Para mejorar el rendimiento, usa embeddings preentrenados y una red neuronal simple.
	•	También puedes probar transformers (como BERT) para una mayor precisión.

Paso 5: Evaluación y Optimización Final

	1.	Evaluación detallada:
	•	Usa una matriz de confusión para visualizar errores comunes y ajustar el modelo si es necesario.

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Crear matriz de confusión
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

	2.	Ajustes finales:
	•	Basado en la matriz de confusión, ajusta el preprocesamiento o prueba otros algoritmos si ciertos errores son recurrentes.

Paso 6: Despliegue del Modelo

	1.	Crear una API de Inferencia:
	•	Usa Flask o FastAPI para crear una API que reciba texto y devuelva la predicción del sentimiento.

from flask import Flask, request, jsonify
import joblib

# Guardar modelo entrenado
joblib.dump(modelo_nb, 'sentiment_model.pkl')

# Crear la API con Flask
app = Flask(__name__)

# Cargar modelo
model = joblib.load('sentiment_model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json(force=True)
    review_text = data['text']
    cleaned_text = limpiar_texto(review_text)  # Usa la función de limpieza que creamos
    vectorized_text = vectorizer.transform([cleaned_text]).toarray()
    prediction = model.predict(vectorized_text)
    return jsonify({'sentiment': prediction[0]})

if __name__ == '__main__':
    app.run(debug=True)

	2.	Despliega en la nube:
	•	Puedes usar Google Cloud Run, AWS Lambda o Heroku para alojar tu API y hacerla accesible de manera pública.

Paso 7: Creación de Dashboard Interactivo

	1.	Dashboard para Visualización de Resultados:
	•	Usa Streamlit o Plotly Dash para crear una interfaz de usuario donde los usuarios puedan escribir una reseña y ver la predicción.

import streamlit as st
import joblib

# Cargar el modelo
model = joblib.load('sentiment_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')  # Cargar vectorizador si se guardó

# Crear la interfaz en Streamlit
st.title("Clasificación de Sentimiento en Reseñas")
review_text = st.text_area("Ingrese su reseña aquí:")

if st.button("Clasificar"):
    cleaned_text = limpiar_texto(review_text)
    vectorized_text = vectorizer.transform([cleaned_text]).toarray()
    prediction = model.predict(vectorized_text)
    st.write(f"Sentimiento Predicho: {prediction[0]}")
 -->]]></content><author><name>Manuel Alejandro Matías Astorga</name></author></entry><entry><title type="html"></title><link href="https://alexmatiasas.github.io/2016-07-20-text-generation/" rel="alternate" type="text/html" title="" /><published>2025-04-20T09:43:17-06:00</published><updated>2025-04-20T09:43:17-06:00</updated><id>https://alexmatiasas.github.io/2016-07-20-text-generation</id><content type="html" xml:base="https://alexmatiasas.github.io/2016-07-20-text-generation/"><![CDATA[<h1 id="text-generator-project">Text generator Project</h1>

<h2 id="project-overview">Project Overview</h2>
<p>This project uses machine learning algorithms to construct a text generator.</p>

<!-- 

-  Construye un generador de texto usando un modelo LSTM o un Transformer.
	•	Ingeniería de características: Trabaja en mejorar la calidad de los datos de entrada.
	•	Tuning de hiperparámetros: Experimenta con la búsqueda de hiperparámetros (Grid Search, Random Search) y técnicas como optimización bayesiana.

 -->

<h2 id="dataset-and-preprocessing">Dataset and Preprocessing</h2>
<p>Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.</p>

<h2 id="model-and-evaluation">Model and Evaluation</h2>
<p>Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.</p>

<!-- // ![ROC Curve](/assets/images/fraud_detection_roc.png) -->

<h2 id="conclusions">Conclusions</h2>
<p>Summarize the results and future steps.</p>]]></content><author><name>Manuel Alejandro Matías Astorga</name></author></entry><entry><title type="html"></title><link href="https://alexmatiasas.github.io/2016-07-20-use-patterns/" rel="alternate" type="text/html" title="" /><published>2025-04-20T09:43:17-06:00</published><updated>2025-04-20T09:43:17-06:00</updated><id>https://alexmatiasas.github.io/2016-07-20-use-patterns</id><content type="html" xml:base="https://alexmatiasas.github.io/2016-07-20-use-patterns/"><![CDATA[<h1 id="use-patterns-project">Use patterns Project</h1>

<h2 id="project-overview">Project Overview</h2>
<p>This project uses machine learning algorithms to detect use patterns.</p>

<!-- 

-  Procesa grandes cantidades de logs de servidor para detectar patrones de uso.
	•	Ingeniería de características: Trabaja en mejorar la calidad de los datos de entrada.
	•	Tuning de hiperparámetros: Experimenta con la búsqueda de hiperparámetros (Grid Search, Random Search) y técnicas como optimización bayesiana.

 -->

<h2 id="dataset-and-preprocessing">Dataset and Preprocessing</h2>
<p>Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.</p>

<h2 id="model-and-evaluation">Model and Evaluation</h2>
<p>Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.</p>

<!-- // ![ROC Curve](/assets/images/fraud_detection_roc.png) -->

<h2 id="conclusions">Conclusions</h2>
<p>Summarize the results and future steps.</p>]]></content><author><name>Manuel Alejandro Matías Astorga</name></author></entry></feed>