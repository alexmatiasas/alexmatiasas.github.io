var store = [{
        "title": "How I'm Building My Data Science Portfolio From Scratch",
        "excerpt":"The Beginning of the Journey   Hi! Iâ€™m Alejandro, a physicist with a deep passion for data-driven problems. After years in academia, Iâ€™ve decided to pivot into Data Science â€” and Iâ€™m building my portfolio one real-world project at a time.   What Iâ€™m Doing   Iâ€™m not just following tutorials â€” Iâ€™m building practical projects that reflect industry-relevant challenges, such as:      ğŸ“Š Sentiment Analysis â€” currently in progress!   ğŸ¤– Object Detection   ğŸ§  Recommendation Engines   ğŸ“¦ ML Pipelines &amp; MLOps   Why Build in Public?   I want to be transparent with my journey. This blog (and my website) will be the place where I showcase not only the results, but also the learning process. That way, employers and collaborators can follow along and see my evolution. a   ğŸ”§ Project Progress                  Project       Status       Progress                       Sentiment Analysis       In Progress       ğŸ”µğŸ”µğŸ”µğŸ”µâšªâšªâšªâšªâšªâšª (40%)                 Object Detection       Planned       ğŸ”µâšªâšªâšªâšªâšªâšªâšªâšªâšª (10%)                 Recommendation System       Planned       âšªâšªâšªâšªâšªâšªâšªâšªâšªâšª (0%)                 Text Generation       Planned       âšªâšªâšªâšªâšªâšªâšªâšªâšªâšª (0%)           Follow the Progress   Feel free to connect with me on LinkedIn or GitHub to follow my work.   Letâ€™s build something great.  ","categories": ["blog"],
        "tags": ["portfolio","career","learning path"],
        "url": "/blog/building-portfolio-from-scratch/",
        "teaser": "/assets/images/teaser.jpg"
      },{
        "title": null,
        "excerpt":"Language model Project   Project Overview  This project uses machine learning algorithms to construct an image classificator.     Dataset and Preprocessing  Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.     Model and Evaluation  Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.     Conclusions  Summarize the results and future steps.  ","categories": [],
        "tags": null,
        "url": "/2016-07-20-image-classification/",
        "teaser": "/assets/images/teaser.jpg"
      },{
        "title": null,
        "excerpt":"Image recognition Project   Project Overview  This project uses machine learning algorithms to construct a recognizer of images.     Dataset and Preprocessing  Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.   Model and Evaluation  Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.     Conclusions  Summarize the results and future steps.  ","categories": [],
        "tags": null,
        "url": "/2016-07-20-image-recognition/",
        "teaser": "/assets/images/teaser.jpg"
      },{
        "title": null,
        "excerpt":"Language model Project   Project Overview  This project uses machine learning algorithms to construct a language model.     Dataset and Preprocessing  Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.   Model and Evaluation  Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.     Conclusions  Summarize the results and future steps.  ","categories": [],
        "tags": null,
        "url": "/2016-07-20-language-model/",
        "teaser": "/assets/images/teaser.jpg"
      },{
        "title": null,
        "excerpt":"Object detection Project   Project Overview  This project uses machine learning algorithms to construct a text generator.     Dataset and Preprocessing  Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.   Model and Evaluation  Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.     Conclusions  Summarize the results and future steps.  ","categories": [],
        "tags": null,
        "url": "/2016-07-20-object-detection/",
        "teaser": "/assets/images/teaser.jpg"
      },{
        "title": null,
        "excerpt":"Real time data processing Project   Project Overview  This project uses machine learning algorithms to analize real time data.     Dataset and Preprocessing  Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.   Model and Evaluation  Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.     Conclusions  Summarize the results and future steps.  ","categories": [],
        "tags": null,
        "url": "/2016-07-20-real-time-data-processing/",
        "teaser": "/assets/images/teaser.jpg"
      },{
        "title": null,
        "excerpt":"Recommendation system Project   Project Overview  This project uses machine learning algorithms to recommend things.     Dataset and Preprocessing  Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.     Model and Evaluation  Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.     Conclusions  Summarize the results and future steps.  ","categories": [],
        "tags": null,
        "url": "/2016-07-20-recomendation-system/",
        "teaser": "/assets/images/teaser.jpg"
      },{
        "title": null,
        "excerpt":"Sentiment Analysis Project   Project Overview  This project analyzes a large dataset of 50,000 movie reviews from IMDb, classifying them into positive or negative sentiments. It is designed to demonstrate NLP preprocessing, exploratory data analysis, and sentiment quantification using R. The project is fully documented in an .Rmd report with visualizations and interactive elements.   ","categories": [],
        "tags": null,
        "url": "/2016-07-20-sentiment-analysis/",
        "teaser": "/assets/images/teaser.jpg"
      },{
        "title": null,
        "excerpt":"Text generator Project   Project Overview  This project uses machine learning algorithms to construct a text generator.     Dataset and Preprocessing  Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.   Model and Evaluation  Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.     Conclusions  Summarize the results and future steps.  ","categories": [],
        "tags": null,
        "url": "/2016-07-20-text-generation/",
        "teaser": "/assets/images/teaser.jpg"
      },{
        "title": null,
        "excerpt":"Use patterns Project   Project Overview  This project uses machine learning algorithms to detect use patterns.     Dataset and Preprocessing  Here you can describe the dataset, preprocessing steps, and challenges in cleaning the data.   Model and Evaluation  Outline the models used and display evaluation metrics like accuracy, precision, recall, etc.     Conclusions  Summarize the results and future steps.  ","categories": [],
        "tags": null,
        "url": "/2016-07-20-use-patterns/",
        "teaser": "/assets/images/teaser.jpg"
      },{
        "title": null,
        "excerpt":"ğŸ¯ Project Overview   This project analyzes a dataset of 50,000 IMDb movie reviews, each labeled as positive or negative. The goal is to classify sentiment through a two-phase workflow:      Exploratory Data Analysis and preprocessing in R   Sentiment classification modeling and deployment in Python   The first phase leverages Râ€™s rich ecosystem for text manipulation and data visualization. The second phase (in progress) will involve training ML models and deploying a classifier using Python.     ğŸ› ï¸ Tools &amp; Libraries      R: tidyverse, tidytext, ggplot2, udpipe, SnowballC, textstem, DT   Python (planned): scikit-learn, nltk, pandas, matplotlib, PyTorch   Techniques: Tokenization, POS tagging, stemming, lemmatization, n-grams, polarity   Format: R Markdown (.Rmd) â†’ HTML (via RPubs)     ğŸ“Š Key Explorations      Sentiment class distribution and review length analysis   HTML cleaning, stopword removal, punctuation &amp; casing normalization   POS tagging using udpipe   Stemming (SnowballC) and lemmatization (textstem)   N-gram analysis for phrase structure insight     ğŸ“ˆ Visualizations Preview   Some of the visualizations in the EDA notebook include:      Word frequency lollipop charts   Sentiment-based word clouds   N-gram distribution (bigrams &amp; trigrams)   Polarity sentiment barplots   ğŸš§ Interactive notebook will be available soon via RPubs.   â¡ï¸ Explore the full interactive report here     ğŸ“˜ Full Exploratory Report   ğŸ› ï¸ The full report with interactive visualizations is currently being compiled and will be published shortly on RPubs.       ğŸ”® Next Steps      Export cleaned data to .csv for model training   Build classifiers using:            Logistic Regression &amp; Naive Bayes (baseline)       Pipeline-based ML models (scikit-learn)       Deep learning model using PyTorch (planned)           Evaluation: Confusion Matrix, F1 Score, ROC-AUC   Optionally deploy via Streamlit or Apache Spark     ğŸ§¾ Deliverables   ğŸ§¾ Deliverables      01_EDA.Rmd â€” Core notebook (R-based)   Cleaned datasets (stemmed, lemmatized, udpipe); export planned for modeling phase   EDA report (RPubs) â€” Coming soon   model_sentiment.py â€” (Coming soon)   Streamlit or Apache Spark deployment â€” (Planned)     ğŸ“Œ Outcome      Completed a robust EDA and text processing pipeline in R.  Laying the foundation for cross-platform sentiment classification with Python.      ğŸ§  What I Learned      R is powerful for quick and elegant EDA and text visualization.   Handling natural language data requires both linguistic and statistical intuition.   Preprocessing choices (e.g., stemming vs lemmatization) can deeply affect downstream model performance.   ğŸ”— View this project on GitHub     ğŸ“Œ Note: This project is currently in Phase 1 (EDA &amp; preprocessing). The modeling and deployment phase will follow shortly.   Last updated: 2025-04-20  ","categories": [],
        "tags": null,
        "url": "/2025-04-21-sentiment-analysis/",
        "teaser": "/assets/images/teaser.jpg"
      },{
        "title": null,
        "excerpt":"ğŸ“Œ Project Overview   This project implements a complete machine learning pipeline to detect fraudulent transactions using the IEEE-CIS dataset. It includes:      Deep EDA in R and Python   Robust preprocessing and feature engineering   Ensemble modeling (XGBoost, LightGBM, CatBoost)   A FastAPI deployment for real-time predictions   Unit testing with pytest for pipeline robustness   ğŸš€ Check the full repository: GitHub     ğŸ” Exploratory Data Analysis   Full notebook: RPubs EDA report   To lay the foundation for modeling, we explored class balance, missingness, temporal patterns and categorical signalsâ€”selecting only the most actionable insights below.     1. Class Imbalance         96.5 % non-fraud vs. 3.5 % fraud   Baseline models were trained on raw data to measure â€œliftâ€ from later oversampling/threshold tuning     2. Missing-Value Patterns         Many id_ &amp; Vxxx features &gt; 85 % missing   Suggests dropping or engineering â€œmissingnessâ€ flags before modeling     3. Categorical Fraud Rates         Product C has ~12 % fraud rate vs. ~3 % overall   Discover cards and mobile devices also show elevated risk     4. Temporal Signal         Fraud peaks around 6 â€“ 9 AM (UTC)   No strong day-of-week effect, but morning flag can improve feature set     Key Takeaways      Severe imbalance (3.5 % fraud) â†’ plan SMOTE/threshold tuning   High missingness in many features â†’ use missingness indicators or drop   Strong categorical drivers (ProductCD, card networks) â†’ prioritize in feature engineering   Temporal window matters â†’ add IsMorning/IsNight flags     ğŸ”§ Data Preprocessing Pipeline   Full notebook: 02_Preprocessing_Modeling_in_Python.ipynb   To prepare the IEEE-CIS dataset for modeling, we built a simple, reproducible pipeline that applies:      ğŸ“¥ Missing-value imputation            Numerical â†’ median       Categorical â†’ most frequent           ğŸ”¤ Categorical encoding            LabelEncoder per column, saved to models/label_encoders.pkl           ğŸ“ Feature scaling            StandardScaler on all numeric features, saved to models/scaler.pkl           ğŸ’¾ Data export            Cleaned DataFrame written to data/processed/train_clean.csv              Pseudocode overview    # 1) Impute df[num_cols] = SimpleImputer('median').fit_transform(df[num_cols]) df[cat_cols] = SimpleImputer('most_frequent').fit_transform(df[cat_cols])  # 2) Encode for col in cat_cols:     df[col] = LabelEncoder().fit_transform(df[col].astype(str))  # 3) Scale df[num_cols] = StandardScaler().fit_transform(df[num_cols])  # 4) Persist transformers and cleaned data joblib.dump(scaler, 'models/scaler.pkl') joblib.dump(encoders, 'models/label_encoders.pkl') df.to_csv('data/processed/train_clean.csv', index=False)      Why this matters      Reproducibility: Same exact logic runs in both notebooks and the FastAPI service.   Modularity: Transformers are versioned via joblib, so you can swap in SMOTE, PCA or any new step later without touching your API.   Performance: Full preprocess on ~590 K Ã— 434 data executes in seconds, keeping your inference path lightweight.     Next up, weâ€™ll load this clean data, split with stratification, and benchmark our first models (Logistic Regression, Random Forest, XGBoost, etc.). Let me know if youâ€™d like that section drafted too!     ğŸ Baseline Models   To set a performance floor, we trained two vanilla classifiers on the fully preprocessed data (no sampling, no feature selection). All scores are stratified 5-fold CV averages:                  Model       Accuracy       Precision       Recall       F1-Score       ROC AUC                       Logistic Regression       0.972       0.825       0.264       0.400       0.856                 Random Forest       0.980       0.942       0.449       0.608       0.933                          Model       ROC-AUC Plot                       Logistic Regression                        Random Forest                     Key takeaways:         Random Forest improves ROC AUC by ~0.08 over Logistic Regression.     Logistic Regression still offers very fast training &amp; simple interpretation.      For a full sideâ€byâ€side of all models (including tuned ensembles and stacking), check out:       âš™ï¸ Phase 2: Advanced Models &amp; Ensembles   After establishing our baseline, we pushed three state-of-the-art gradient boosters through hyperparameter tuning, and then combined them via two ensemble strategies. All scores are stratified 5-fold CV averages on the preprocessed data.     1 XGBoost   We initialized XGBoost with common defaults and then we would tuned via GridSearchCV:   xgb = XGBClassifier(     n_estimators=200,     max_depth=6,     learning_rate=0.1,     subsample=0.8,     colsample_bytree=0.8,     use_label_encoder=False,     eval_metric='logloss',     random_state=42 )   Key metrics      ğŸš€ ROC AUC: 0.9234   ğŸ¯ Precision (fraud): 0.957   ğŸ” Recall (fraud): 0.589   ğŸ›ï¸ F1-Score (fraud): 0.729   Observations:      Strong out-of-the-box performance, beating Logistic Regression by +0.07 AUC.   High precision means few false alarms; recall (0.59) leaves room for further improvement via oversampling.   2 LightGBM   LightGBM is a fast, memory-efficient gradient boosting framework:   lgb = LGBMClassifier(     n_estimators=200,     max_depth=6,     learning_rate=0.1,     subsample=0.8,     colsample_bytree=0.8,     random_state=42 )   Key metrics      ğŸš€ ROC AUC: 0.9157   ğŸ¯ Precision (fraud): 0.880   ğŸ” Recall (fraud): 0.410   ğŸ›ï¸ F1-Score (fraud): 0.560   Observations:      Slightly lower AUC than XGBoost but still highly discriminative.   Very fast training on large dataâ€”ideal for rapid iteration.   3 CatBoost   CatBoost natively handles categorical features and counteracts imbalance efficiently:   cb = CatBoostClassifier(     iterations=500,     learning_rate=0.1,     depth=6,     random_state=42,     verbose=False )   Key metrics      ğŸš€ ROC AUC: 0.9174   ğŸ¯ Precision (fraud): 0.946   ğŸ” Recall (fraud): 0.497   ğŸ›ï¸ F1-Score (fraud): 0.651   Observations:      Competitive AUC close to LightGBM with minimal encoding effort.   Good balance of precision/recall out of the box.   In the next phase, weâ€™ll perform hyperparameter tuning to maximize our classification performanceâ€”focusing especially on the F1-score, which is crucial when the positive (fraud) class is both rare and of primary interest.   âš™ï¸ Phase 3: Hyperparameter Tuning   In this phase, we applied grid and randomized searches to each of our core modelsâ€”Logistic Regression, Random Forest, XGBoost, LightGBM and CatBoostâ€”using a small subsample for speed, then retrained the best estimators on the full data to measure real-world performance.                  Model       Best Parameters       ROC AUC       Precision (fraud)       Recall (fraud)       F1-Score (fraud)                       Logistic Regression       C=0.01, penalty='l2', solver='liblinear'       0.818       0.825       0.264       0.400                 Random Forest       n_estimators=300, max_depth=20, min_samples_split=2, min_samples_leaf=2, max_features='sqrt'       0.911       0.940       0.370       0.530                 XGBoost       subsample=0.8, n_estimators=500, max_depth=10, learning_rate=0.05, colsample_bytree=0.6, gamma=0, min_child_weight=1       0.969       0.957       0.589       0.729                 LightGBM       subsample=0.6, n_estimators=500, max_depth=10, learning_rate=0.1, colsample_bytree=1.0, num_leaves=63, min_child_samples=10       0.964       0.930       0.620       0.740                 CatBoost       iterations=500, depth=8, learning_rate=0.1, border_count=128, l2_leaf_reg=1, bagging_temperature=0       0.960       0.950       0.500       0.651           ğŸ” Key Takeaways      Defaults were strong: Logistic Regression actually saw a drop in AUC after tuning, indicating its default C=1 was near-optimal for our data.   Tree-based gains: XGBoost tuning delivered the best AUC (0.969), closely followed by LightGBM and CatBoost.   Precision vs. recall: All tuned boosters maintain very high precision (&gt;0.93) but recall ranges from 0.50 (CatBoost) to 0.62 (LightGBM), suggesting further imbalance strategies could improve fraud capture.   ğŸ”— Phase 4: Ensemble Models   In the final stage, we combined our top-tuned boosters into two ensemble strategies to maximize robustness and performance.     1 Soft Voting Ensemble   We created a VotingClassifier that averages the predicted probabilities of our three best models:   from sklearn.ensemble import VotingClassifier  ensemble_voting = VotingClassifier(     estimators=[         (\"xgb\", best_xgb_model),         (\"lgb\", best_lgb_model),         (\"cat\", best_cat_model)     ],     voting=\"soft\",      # average probabilities     weights=[1, 1, 1],  # equal weighting     n_jobs=-1 )  ensemble_voting.fit(X_train, y_train)   Voting Ensemble Metrics      ğŸš€ ROC AUC: 0.9648   ğŸ¯ Precision (fraud): 0.955   ğŸ” Recall (fraud): 0.571   ğŸ›ï¸ F1-Score (fraud): 0.715   2 Stacking Ensemble   Next, we used a StackingClassifier to learn optimal combinations of base-model outputs, with a LogisticRegression metaâ€learner:   from sklearn.ensemble import StackingClassifier from sklearn.linear_model import LogisticRegression  stacking = StackingClassifier(     estimators=[         (\"xgb\", best_xgb_model),         (\"lgb\", best_lgb_model),         (\"cat\", best_cat_model)     ],     final_estimator=LogisticRegression(max_iter=1000),     passthrough=False,     cv=5,     n_jobs=-1 )  stacking.fit(X_train, y_train)   Stacking Ensemble Metrics      ğŸš€ ROC AUC: 0.9691   ğŸ¯ Precision (fraud): 0.924   ğŸ” Recall (fraud): 0.679   ğŸ›ï¸ F1-Score (fraud): 0.783   Therefore, the stacking ensemble emerges as the clear winnerâ€”as shown in our baseline comparison table.     ğŸ§ª Testing   We implemented tests to ensure pipeline integrity:      âœ… Preprocessing pipeline does not crash with valid data   âœ… API responds with expected output structure   âœ… Unit tests managed with pytest     ğŸ–¥ï¸ Deployment   The final model was locally deployed using FastAPI. Key features:      /predict endpoint returns prediction and fraud probability   Interactive Swagger UI available at /docs   Can be run locally via Uvicorn or deployed in a Docker container   uvicorn src.main:app --reload   Or with Docker   docker pull alexmatiasastorga/fraud-api:latest docker run -d -p 8000:8000 alexmatiasastorga/fraud-api   ğŸ“Œ Conclusion   This project demonstrates a real-world machine learning workflow from raw data to deployment. Future improvements may include:      DAG automation with Apache Airflow   Cloud deployment (Render or AWS)   Monitoring with MLFlow or Prometheus   ğŸ”— Links      ğŸ” Project Repository   ğŸ³ Docker Image  ","categories": [],
        "tags": null,
        "url": "/2025-05-12-fraud-detection/",
        "teaser": "/assets/images/teaser.jpg"
      },,{
    "title": "About Me",
    "excerpt":"Hi, Iâ€™m Alejandro â€” a Physicist turned Data Scientist with a passion for solving complex problems through data. From quarks to queries, my journey has taken me from academic research to the world of Machine Learning and AI.    Technical Skills   Programming Languages                   Python        R       Â        Â                   SQL:        (Familiar with) PostgresSQL        (Familar with) MySQL        (Familiar with) SQLite                  NoSQL:        (Familiar with) MongoDB       Â        Â              Data Manipulation &amp; Processing                  Libraries:        NumPy        Pandas             Cloud &amp; Deployment Platforms                  Cloud Platforms:         (Familiar with) Amazon Web Services (AWS)                 Containerization &amp; Orchestration:        (Familiar with) Docker             Machine Learning &amp; AI                  Libraries:        (Some experience) Scikit-learn        (Some experience) TensorFlow        (Familiar with) PyTorch           Algorithms      Supervised Learning:                  Linear Regression       Logistic Regression       Decision Trees       Random Forest       Support Vector Machines (SVM)       Neural Networks              Unsupervised Learning:                  Clustering       Principal Component Analysis (PCA)       Autoencoders              Reinforcement Learning:                  Q-learning       Deep Q-Networks       Policy Gradient Methods           Data Visualization                  Libraries:        Matplotlib        Seaborn        Plotly        ggplot2                 Dashboard Tools:       Tableau        PowerBI       Â        Â            MLOps &amp; Workflow Orchestration                  Version Control:        Git        GitHub        (Familiar with) Data Version Control (DVC)                 Pipeline Orchestration:        (Planning to work with) Airflow        (Planning to work with) Kubeflow       Â               Soft Skills  Problem-solving | Scientific communication | Research mindset | Self-taught learner | Team collaboration    Education   PhD in Physics                  PhD in Physics       CINVESTAV - Physics Department                       2019 - 2024       Mexico city, Mexico                 Â        Researched the internal structure of hadrons using the MIT Bag Model and non-extensive Tsallis statistics. Developed a thermodynamically consistent model of quark-gluon matter to study pressure distribution inside the proton.           Physics Engineer                  Physics Engineer       Technological University of the Mixteca - Physics Department                       2013 - 2018       Huajuapan de LeÃ³n, Oaxaca - Mexico                 Â        Focused on theoretical physics. I worked in a new model of population growth.           Certifications      Complete Python Bootcamp (Udemy, 2022â€“2023) â€” JosÃ© Portilla, Pierian Training   SQL Bootcamp: Zero to Hero (Udemy, 2022â€“2023) â€” JosÃ© Portilla   Machine Learning A-Z: AI, Python &amp; R (Udemy, 2023â€“2024) â€” Kirill Eremenko, Hadelin de Ponteves   AI A-Z (2024): Build 7 AI + LLM Models (Udemy, 2023â€“2024) â€” Kirill Eremenko, Hadelin de Ponteves    Featured Projects   Here are some of the projects I plan to build and showcase soon. ğŸš§ Work In Progress   Stay tuned!                                                                                                                                                                           Built a sentiment analysis model to classify text as positive or negative, useful for customer feedback and social media analysis.                                                   Read More                                                                                                                                                            Fraud Detection                                                    Machine Learning model to detect fraudulent transactions using ensemble methods.                                                   Read More                                                                                                                                                            Real-Time Object Detection                                                    Placeholder for YOLO-based real-time object detection pipeline on video streams.                                                   Coming Soon                                                                                                                                                            Recommendation System                                                    Placeholder for collaborative filtering recommendation engine.                                                   Coming Soon                                           Letâ€™s Connect   Feel free to reach out if youâ€™re interested in collaborating, have a question, or just want to say hi!      ğŸ“§ Email: alejandromatiasastorga@gmail.com   ğŸ’¼ LinkedIn: alexmatiasastorga   ğŸ§ª GitHub: alexmatiasas  ","url": "https://alexmatiasas.github.io/about/"
  },{
    "title": "Blog",
    "excerpt":" ","url": "https://alexmatiasas.github.io/blog/"
  },{
    "title": "Categories",
    "excerpt":"Explore my projects and blog posts by category.  ","url": "https://alexmatiasas.github.io/categories/"
  },{
    "title": "Contact",
    "excerpt":" Feel free to reach out through LinkedIn or send me an email at alejandromatiasastorga@gmail.com. Iâ€™m always open to discussing new projects, creative ideas, or opportunities to be part of your vision.   Download my CV   Contact form          Your name:       Your email:       Message:       Send    ","url": "https://alexmatiasas.github.io/contact/"
  },{
    "title": "Welcome to My Data Science Portfolio!",
    "excerpt":"Hi, Iâ€™m Alejandro!   Iâ€™m a Data Scientist and Machine Learning Engineer with a PhD in Physics, specializing in using data and code to solve complex problems. My expertise blends scientific research with modern machine learning practices, and Iâ€™m currently transitioning my academic rigor into scalable, data-driven solutions.      Contact Me   Connect on LinkedIn                                                                                                                        Projects                                                    Explore a growing collection of real-world applications in Data Science, from Sentiment Analysis to Fraud Detection.                                                   View Projects                                                                                                                                                    Blog                                                    Technical breakdowns, insights, and tutorials from my journey in ML and Data Science.                                                   Read Blog                                                                                                                                                     About Me                                                    Get to know more about my academic background and transition into Data Science.                                                   About Me                                         Featured projects   These are placeholders for my in-progress portfolio. Visit the Projects page for updates as they are completed.                                                                                                                       Sentiment Analysis                                                    Built a sentiment analysis model to classify text as positive or negative, useful for customer feedback and social media analysis.                                                   Read More                                                                                                                                                            Fraud Detection                                                    Machine Learning model to detect fraudulent transactions using ensemble methods.                                                   Read More                                                                                                                                                            Real-Time Object Detection                                                    Placeholder for YOLO-based real-time object detection pipeline on video streams.                                                   Coming Soon                                                                                                                                                            Recommendation System                                                    Placeholder for collaborative filtering recommendation engine.                                                   Coming Soon                                           Note: These projects are currently under active development. Iâ€™m building this portfolio to reflect real, practical experience in Data Science and Machine Learning. Stay tuned for updates!     Manuel Alejandro MatÃ­as Astorga | Data Science Portfolio â€“ Machine Learning &amp; AI Projects ","url": "https://alexmatiasas.github.io/"
  },{
    "title": "Projects",
    "excerpt":"## ğŸ”§ Project Progress  | Project                  | Status        | Progress        | |--------------------------|---------------|-----------------| | Sentiment Analysis       | In Progress   | ğŸ”µğŸ”µğŸ”µğŸ”µâšªâšªâšªâšªâšªâšª (40%) | | Object Detection         | Planned       | ğŸ”µâšªâšªâšªâšªâšªâšªâšªâšªâšª (10%) | | Recommendation System    | Planned       | âšªâšªâšªâšªâšªâšªâšªâšªâšªâšª (0%)  | | Text Generation          | Planned       | âšªâšªâšªâšªâšªâšªâšªâšªâšªâšª (0%)  |  # Natural Language Processing (NLP)  ## Language Model for Text Generation  {% include feature_row id=\"language-model-for-text-generation\" type=\"left\" %}    ## Sentiment Analysis  {% include feature_row id=\"sentiment-analysis\" type=\"left\" %}  ## Custom Language Model    {% include feature_row id=\"language-model\" type=\"left\" %}  # Computer Vision  ## Image Classification  {% include feature_row id=\"image-classification\" type=\"right\" %}  ## Image Recognition  {% include feature_row id=\"image-recognition\" type=\"right\" %}  ## Object Detection System  {% include feature_row id=\"real-time-object-detection\" type=\"right\" %}  # Recommendation Systems  ## Customer Recommendation System  {% include feature_row id=\"customer-recommendation-system\" type=\"left\" %}  # Real-Time Data Processing  ## Real-Time Data Processing Pipeline  {% include feature_row id=\"real-time-data-processing\" type=\"right\" %}  # Data Patterns and Analytics  ## Fraud Detection  {% include feature_row id=\"fraud-detection\" type=\"left\" %}  ## Use Patterns Analysis  {% include feature_row id=\"use-patterns-analysis\" type=\"left\" %}","url": "https://alexmatiasas.github.io/projects/"
  },{
    "title": "Tags",
    "excerpt":"Find content by tags across my Data Science and Machine Learning portfolio.","url": "https://alexmatiasas.github.io/tags/"
  },{
    "title": "Thanks for Your Message!",
    "excerpt":"Thanks for reaching out! Iâ€™ll get back to you as soon as possible.   In the meantime, feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/alexmatiasastorga/).","url": "https://alexmatiasas.github.io/thank-you/"
  }]
